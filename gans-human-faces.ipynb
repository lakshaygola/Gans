{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install jovian --upgrade --quiet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torchvision.transforms as t\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import ImageFolder\nimport os\nfrom torchvision.utils import make_grid\nimport torch.nn as nn\nimport torch.functional as F\nimport torch.nn.functional as ff\nfrom tqdm.notebook import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setting up the data directory and loading the dataset\ndataset= '../input/flickrfaceshq-dataset-ffhq'\ndata_dir= '../input'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir(data_dir)[:15])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalizing parameter, batch size, image size\nstats= (0.5,0.5,0.5), (0.5,0.5,0.5)\nbatch_size= 150\nimage_size= 64","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Appling the transformation and normalization on the image data\nimage_data= ImageFolder(data_dir, transform= t.Compose([t.Resize(image_size), t.CenterCrop(image_size), t.ToTensor(), t.Normalize(*stats)]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making data loader from the dataset\nimagedata_loader= DataLoader(image_data, batch_size, pin_memory= 4, num_workers= True, shuffle= True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Vizualise the images of the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to normalization\ndef denorm(image):\n    return image * stats[0][1] + stats[0][1]\n\n# Function to make the grid of the images\ndef grid_frame(image, nums= 70):\n    fig, ax= plt.subplots(figsize= (12,10))\n    ax.set_xticks([]), ax.set_yticks([])\n    ax.imshow(make_grid(denorm(image.detach()[:nums]), nrow= 10).permute(1,2,0))\n    \n# Function to make the grid of batches\ndef make_batch(imagedata_loader, nums= 70):\n    for image, _ in imagedata_loader:\n        grid_frame(image)\n        break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Example images from the data set\nmake_batch(imagedata_loader)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### To work with GPUs"},{"metadata":{"trusted":true},"cell_type":"code","source":"# To clear the cache\ntorch.cuda.empty_cache()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to return the available device\ndef get_device():\n    if torch.cuda.is_available():\n        return ('cuda')\n    else:\n        return ('cpu')\n    \n# Function to move the data on avialable device\ndef to_device(data, device):\n    if isinstance(data, (list, tuple)):\n        return [to_device(xdata, device) for xdata in data]\n    else:\n        return data.to(device, non_blocking= True)\n    \n# Class to move the batch of data in avialable device\nclass devicedataloader():\n    def __init__(self, imagedata_loader, device):\n        self.dl = imagedata_loader\n        self.device= device\n    \n    def __iter__(self):\n        for batch in self.dl:\n            yield to_device(batch, self.device)\n            \n    def __len__(self):\n        return len(self.dl)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device= get_device()\ndevice","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.cuda.empty_cache()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Discriminator \nDiscriminator used to predict weather it belong to dataset or generated using generated model"},{"metadata":{"trusted":true},"cell_type":"code","source":"discriminator= nn.Sequential(\n    nn.Conv2d(3, 64, kernel_size=4 , stride= 2, padding= 1, bias= False),\n    nn.BatchNorm2d(64),\n    nn.LeakyReLU(0.2, inplace= True),\n    \n    nn.Conv2d(64, 128, kernel_size=4 , stride= 2, padding= 1, bias= False),\n    nn.BatchNorm2d(128),\n    nn.LeakyReLU(0.2, inplace= True),\n    \n    nn.Conv2d(128, 256, kernel_size=4 , stride= 2, padding= 1, bias= False),\n    nn.BatchNorm2d(256),\n    nn.LeakyReLU(0.2, inplace= True),\n    \n    nn.Conv2d(256, 512, kernel_size=4 , stride= 2, padding= 1, bias= False),\n    nn.BatchNorm2d(512),\n    nn.LeakyReLU(0.2, inplace= True),\n    \n    nn.Conv2d(512, 1, kernel_size= 4, stride= 1, padding= 0, bias= False),\n    \n    nn.Flatten(),\n    nn.Sigmoid()\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Generator Function\nGenerator is used to generate the image using the image dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining the latent size (Number of images in one batch)\nlatent_size= 130","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"generator= nn.Sequential(\n    nn.ConvTranspose2d(latent_size, 512, kernel_size= 4, stride= 1, padding= 0, bias= False),\n    nn.BatchNorm2d(512),\n    nn.ReLU(inplace= True),\n    \n    nn.ConvTranspose2d(512, 256, kernel_size= 4, stride= 2, padding= 1, bias= False),\n    nn.BatchNorm2d(256),\n    nn.ReLU(inplace= True),\n    \n    nn.ConvTranspose2d(256, 128, kernel_size= 4, stride= 2, padding= 1, bias= False),\n    nn.BatchNorm2d(128),\n    nn.ReLU(inplace= True),\n    \n    nn.ConvTranspose2d(128, 64, kernel_size= 4, stride= 2, padding= 1, bias= False),\n    nn.BatchNorm2d(64),\n    nn.ReLU(inplace= True),\n    \n    nn.ConvTranspose2d(64, 3, kernel_size= 4, stride= 2, padding= 1, bias= False),\n    nn.Tanh()\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Moving data loader to the avialable device\nimagedata_loader= devicedataloader(imagedata_loader, device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Testing the genertor model (is it working or not)\nxb= torch.randn(batch_size, latent_size, 1 , 1)      # Batch of the images of size latent x 1 x 1\nfake_image= generator(xb)           # Creating fake image using generator\ngrid_frame(fake_image)\nprint(fake_image.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Moving discriminator and generator on the availabel device\nto_device(discriminator, device)\nto_device(generator, device)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training Discriminator and generator "},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_dis(images, opt_d):\n    \n    # Reseting the gradinet\n    opt_d.zero_grad()\n    \n    # Target for the real image is set to be 1 \n    real_pred= discriminator(images)\n    real_target= torch.ones(images.size(0), 1, device= device)\n    real_loss= ff.binary_cross_entropy(real_pred, real_target)\n    real_score= torch.mean(real_loss).item()\n    \n    # Generating the fake images and setting the label as 0\n    latent_vector= torch.randn(batch_size, latent_size, 1, 1, device= device)\n    fake_image= generator(latent_vector)\n    \n    fake_image_pred= discriminator(fake_image)\n    fake_target= torch.zeros(fake_image.size(0), 1, device= device)\n    fake_loss= ff.binary_cross_entropy(fake_image_pred, fake_target)\n    fake_score= torch.mean(fake_loss).item()\n    \n    # Total loss and gradient \n    loss= fake_loss + real_loss\n    loss.backward()\n    opt_d.step()\n    \n    return loss.item(), real_score, fake_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_generator(opt_g):\n    opt_g.zero_grad()\n    \n    latent_vector= torch.randn(batch_size, latent_size, 1, 1, device= device)\n    \n    fake_images= generator(latent_vector)\n    \n    # Taking prediction on fake images we generated using generator and setting target as 1\n    fake_pred= discriminator(fake_images)\n    target= torch.ones(fake_images.size(0), 1, device= device)\n    loss= ff.binary_cross_entropy(fake_pred, target)\n    \n    loss.backward()\n    opt_g.step()\n    \n    return loss.item()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Defining fit function to train whole model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def fit(epochs, lr, idx= 1):\n    \n    # Creating list to track the performance of the model\n    losses_gen= []\n    losses_dis= []\n    real_score= []\n    fake_score= []\n    \n    # Initalizing the optimizer\n    opt_d= torch.optim.Adam(discriminator.parameters(), lr, betas= (0.5, 0.9))\n    opt_g= torch.optim.Adam(generator.parameters(), lr, betas= (0.5, 0.9))\n    \n    # Train the model for epochs\n    for epoch in range(epochs):\n        \n        for real_images, _ in tqdm(imagedata_loader):\n            loss_d, real_s, fake_s= train_dis(real_images, opt_d)\n            loss_g= train_generator(opt_g)\n            \n        # Storing the losses\n        losses_gen.append(loss_g)\n        losses_dis.append(loss_d)\n        real_score.append(real_s)\n        fake_score.append(fake_s)\n        \n        print('Epochs: [{}], Generator loss: {:.4f}, Discriminator loss: {:.4f}, Real Score: {:.4f}, Fake Score: {:.4f}'.\n              format(epoch, loss_g, loss_d, real_s, fake_s))\n        \n    return losses_gen, losses_dis, real_score, fake_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now saving the progress of our model in the form of image"},{"metadata":{"trusted":true},"cell_type":"code","source":"from torchvision.utils import save_image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Images from which we track the progress of our model\nlatent_tracker= torch.randn(batch_size, latent_size, 1, 1, device= device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating the directory to save all those images which is used to see the progress of the model\ntracking_dir= 'generated'\nos.makedirs(tracking_dir, exist_ok= True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining the function to save the image and also plot the images\ndef saving(latent, index, show= True):\n    fake_image= generator(latent)\n    fimage_name= 'generated-images-{0:0=4d}.png'.format(index)\n    save_image(denorm(fake_image), os.path.join(tracking_dir, fimage_name), nrow= 10)\n    print('Saving', fimage_name)\n    if show:\n        fig, ax= plt.subplots(figsize= (10,15))\n        ax.set_xticks([]), ax.set_yticks([])\n        plt.imshow(make_grid(fake_image.cpu().detach(), nrow= 10).permute(1,2,0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"saving(latent_tracker, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"import jovian\njovian.commit(project= 'humans-faces-gans')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Definig hyperparameters\nlr= 0.002\nepochs= 30","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jovian.log_hyperparams(learning_rate= lr, No_of_epochs= epochs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jovian.log_metrics(Batch_size= batch_size, Latent_size= latent_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets train the model then we will see the progress\n0history= fit(epochs, lr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"saving(latent_tracker, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the graph to see the change in the loss after each epoch\nlosses= []\nplt.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}